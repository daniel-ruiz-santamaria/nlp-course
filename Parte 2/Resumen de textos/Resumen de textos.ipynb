{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Resumen de textos.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9NKZSLBajS6pjyT9W3C+7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Resumen de Textos con NLP"],"metadata":{"id":"y0QNmZU568Aa"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"taoaUQ9A7HA7"}},{"cell_type":"code","source":["!pip install beautifulsoup4\n","!pip install lxml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QWeWOxVz7JWH","executionInfo":{"status":"ok","timestamp":1647793158768,"user_tz":-60,"elapsed":6341,"user":{"displayName":"Daniel Ruiz Santamaría","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00829749213257681441"}},"outputId":"49725aa8-d5eb-4f61-998e-784093f340a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"OVFCnh6L68Fs"}},{"cell_type":"code","source":["import bs4 as bs\n","import urllib.request\n","import re"],"metadata":{"id":"ROE6b5Jp7N9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["URL_INGLES = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n","URL_ESPAÑOL = 'https://es.wikipedia.org/wiki/Procesamiento_de_lenguajes_naturales'\n","scraped_data = urllib.request.urlopen(URL_ESPAÑOL)\n","article = scraped_data.read()\n","\n","parsed_article = bs.BeautifulSoup(article,'lxml')\n","\n","paragraphs = parsed_article.find_all('p')\n","\n","article_text = \"\"\n","\n","for p in paragraphs:\n","    article_text += p.text"],"metadata":{"id":"U6mMDtPy7WJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["article_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"shqnuhna7ZaH","executionInfo":{"status":"ok","timestamp":1647793158770,"user_tz":-60,"elapsed":27,"user":{"displayName":"Daniel Ruiz Santamaría","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00829749213257681441"}},"outputId":"86ac08d8-1276-4d0f-b44d-f02b8ffec62a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'El procesamiento de lenguaje natural,[1]\\u200b[2]\\u200b abreviado PLN[3]\\u200b[4]\\u200b —en inglés, natural language processing, NLP— es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo. No trata de la comunicación por medio de lenguas naturales de una forma abstracta, sino de diseñar mecanismos para comunicarse que sean eficaces computacionalmente —que se puedan realizar por medio de programas que ejecuten o simulen la comunicación—. Los modelos aplicados se enfocan no solo a la comprensión del lenguaje de por sí, sino a aspectos generales cognitivos humanos y a la organización de la memoria. El lenguaje natural sirve solo de medio para estudiar estos fenómenos. Hasta la década de 1980, la mayoría de los sistemas de PLN se basaban en un complejo conjunto de reglas diseñadas a mano. A partir de finales de 1980, sin embargo, hubo una revolución en PLN con la introducción de algoritmos de aprendizaje automático para el procesamiento del lenguaje.[5]\\u200b[6]\\u200b\\nLa historia del PLN empieza desde 1950, aunque se han encontrado trabajos anteriores. En 1950, Alan Turing publicó Computing machinery and intelligence, donde proponía lo que hoy se llama el test de turing como criterio de inteligencia. En 1954, el experimento de Georgetown involucró traducción automática de más de sesenta oraciones del ruso al inglés. Los autores sostuvieron que en tres o cinco años la traducción automática sería un problema resuelto. El avance real en traducción automática fue más lento, y en 1966 el reporte ALPAC demostró que la investigación había tenido un bajo desempeño. Más tarde, hasta finales de 1980, se llevaron a cabo investigaciones a menor escala en traducción automática, y se desarrollaron los primeros sistemas de traducción automática estadística. Esto se debió tanto al aumento constante del poder de cómputo resultante de la ley de Moore como a la disminución gradual del predominio de las teorías lingüísticas de Noam Chomsky (por ejemplo, la gramática transformacional), cuyos fundamentos teóricos desalentaron el tipo de lingüística de corpus, que se basa en el enfoque de aprendizaje de máquinas para el procesamiento del lenguaje. Se usaron entonces los primeros algoritmos de aprendizaje automático, como los árboles de decisión, sistemas producidos de sentencias si-entonces similares a las reglas escritas a mano. Se puede consultar un resumen de la historia de 50 años de publicaciones acerca del procesamiento automático después del proyecto NLP4NLP en una publicación doble en Frontiers in Research Metrics and Analytics.[7]\\u200b[8]\\u200b\\nLas lenguas naturales son inherentemente ambiguas en diferentes niveles:\\nPara resolver estos tipos de ambigüedades y otros, el problema central en el PLN es la traducción de entradas en lenguas naturales a una representación interna sin ambigüedad, como árboles de análisis.\\nEn la lengua hablada no se suelen hacer pausas entre palabra y palabra. El lugar en el que se deben separar las palabras a menudo depende de cuál es la posibilidad de que mantenga un sentido lógico tanto gramatical como contextual. En la lengua escrita, lenguas como el chino mandarín tampoco tienen separaciones entre las palabras.\\nAcentos extranjeros, regionalismos o dificultades en la producción del habla, errores de mecanografiado o expresiones no gramaticales, errores en la lectura de textos mediante OCR\\nLas principales tareas de trabajo en el PLN son:\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","source":["## Proprocesamiento"],"metadata":{"id":"7QoO00797z10"}},{"cell_type":"code","source":["# Removing Square Brackets and Extra Spaces\n","article_text = re.sub(r'[[0-9]*]', ' ', article_text) # Borrar los números entre corchetes\n","article_text = re.sub(r'\\s+', ' ', article_text) # Borrar los espacios extra dejando solo un espacio"],"metadata":{"id":"HdG_iinY735n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing special characters and digits\n","formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n","formatted_article_text = re.sub(r's+', ' ', formatted_article_text)"],"metadata":{"id":"a2L1nxgu8SsH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Convertir el texto a frases"],"metadata":{"id":"chUEMKN18dsq"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","sentence_list = nltk.sent_tokenize(article_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVXzpFP48Bfu","executionInfo":{"status":"ok","timestamp":1647793158772,"user_tz":-60,"elapsed":27,"user":{"displayName":"Daniel Ruiz Santamaría","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00829749213257681441"}},"outputId":"ba6f99bb-d38a-485e-9ad8-78786929048c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## Eliminación de Stop Words"],"metadata":{"id":"QQVejwX2C8eG"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words('spanish')\n","\n","word_frequencies = {} # Diccionario de frecuencias\n","for word in nltk.word_tokenize(formatted_article_text): # Para cada frase, tokenizar e iterar por palabra\n","    if word not in stopwords: # Si no esta en la lista de Stop Words\n","        if word not in word_frequencies.keys(): # Si la palabra no esta en el diccionario\n","            word_frequencies[word] = 1 # La añado e inicializo la frecuencia a 1\n","        else:\n","            word_frequencies[word] += 1 # Si esta, aumento la frecuencia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TTpXHURDSJy","executionInfo":{"status":"ok","timestamp":1647793158773,"user_tz":-60,"elapsed":26,"user":{"displayName":"Daniel Ruiz Santamaría","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00829749213257681441"}},"outputId":"f849b938-c8fa-4daf-8a79-b6453d29a1b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["print(word_frequencies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wytXK4a0Dvst","executionInfo":{"status":"ok","timestamp":1647793158774,"user_tz":-60,"elapsed":26,"user":{"displayName":"Daniel Ruiz Santamaría","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00829749213257681441"}},"outputId":"20183e26-c276-44ba-def8-2813d3c8ce94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'El': 4, 'proce': 5, 'amiento': 4, 'lenguaje': 7, 'natural': 4, 'abreviado': 1, 'PLN': 6, 'ingl': 2, 'language': 1, 'ing': 1, 'NLP': 3, 'campo': 1, 'ciencia': 1, 'computaci': 1, 'n': 23, 'inteligencia': 2, 'artificial': 1, 'ling': 3, 'tica': 10, 'tudia': 1, 'interaccione': 1, 'computadora': 1, 'humano': 2, 'Se': 3, 'ocupa': 1, 'formulaci': 1, 'inve': 3, 'tigaci': 2, 'mecani': 2, 'mo': 3, 'eficace': 2, 'computacionalmente': 2, 'comunicaci': 3, 'per': 1, 'ona': 1, 'm': 4, 'quina': 2, 'medio': 4, 'decir': 1, 'lengua': 7, 'mundo': 1, 'No': 1, 'trata': 1, 'naturale': 3, 'forma': 1, 'ab': 1, 'tracta': 1, 'ino': 2, 'di': 3, 'ar': 1, 'comunicar': 1, 'ean': 1, 'puedan': 1, 'realizar': 1, 'programa': 1, 'ejecuten': 1, 'imulen': 1, 'Lo': 2, 'modelo': 1, 'aplicado': 1, 'enfocan': 1, 'olo': 2, 'compren': 1, 'i': 7, 'pecto': 1, 'generale': 1, 'cognitivo': 1, 'organizaci': 1, 'memoria': 1, 'irve': 1, 'tudiar': 1, 'to': 3, 'fen': 1, 'meno': 1, 'Ha': 1, 'ta': 2, 'd': 1, 'cada': 1, 'mayor': 1, 'tema': 3, 'ba': 2, 'aban': 1, 'complejo': 1, 'conjunto': 1, 'regla': 2, 'ada': 1, 'mano': 2, 'A': 1, 'partir': 1, 'finale': 2, 'in': 3, 'embargo': 1, 'revoluci': 1, 'introducci': 1, 'algoritmo': 2, 'aprendizaje': 3, 'autom': 8, 'tico': 3, 'La': 3, 'hi': 2, 'toria': 2, 'empieza': 1, 'aunque': 1, 'encontrado': 1, 'trabajo': 2, 'anteriore': 1, 'En': 4, 'Alan': 1, 'Turing': 1, 'public': 1, 'Computing': 1, 'machinery': 1, 'and': 2, 'intelligence': 1, 'propon': 1, 'hoy': 1, 'llama': 1, 't': 1, 'turing': 1, 'criterio': 1, 'experimento': 1, 'Georgetown': 1, 'involucr': 1, 'traducci': 6, 'enta': 1, 'oracione': 1, 'ru': 1, 'autore': 1, 'tre': 1, 'cinco': 1, 'er': 1, 'problema': 2, 're': 4, 'uelto': 1, 'avance': 1, 'real': 1, 'lento': 1, 'reporte': 1, 'ALPAC': 1, 'demo': 1, 'tr': 1, 'hab': 1, 'bajo': 1, 'empe': 1, 'M': 1, 'tarde': 1, 'llevaron': 1, 'cabo': 1, 'tigacione': 1, 'menor': 1, 'cala': 1, 'arrollaron': 1, 'primero': 2, 'tad': 1, 'E': 1, 'debi': 1, 'aumento': 1, 'tante': 1, 'poder': 1, 'c': 1, 'mputo': 1, 'ultante': 1, 'ley': 1, 'Moore': 1, 'minuci': 1, 'gradual': 1, 'predominio': 1, 'teor': 1, 'Noam': 1, 'Chom': 1, 'ky': 1, 'ejemplo': 1, 'gram': 1, 'tran': 1, 'formacional': 1, 'cuyo': 1, 'fundamento': 1, 'rico': 1, 'alentaron': 1, 'tipo': 2, 'corpu': 1, 'enfoque': 1, 'u': 1, 'aron': 1, 'entonce': 2, 'rbole': 2, 'deci': 1, 'producido': 1, 'entencia': 1, 'imilare': 1, 'crita': 2, 'puede': 1, 'ultar': 1, 'umen': 1, 'publicacione': 1, 'acerca': 1, 'pu': 1, 'proyecto': 1, 'publicaci': 1, 'doble': 1, 'Frontier': 1, 'Re': 1, 'earch': 1, 'Metric': 1, 'Analytic': 1, 'on': 2, 'inherentemente': 1, 'ambigua': 1, 'diferente': 1, 'nivele': 1, 'Para': 1, 'olver': 1, 'ambig': 2, 'edade': 1, 'central': 1, 'entrada': 1, 'repre': 1, 'entaci': 1, 'interna': 1, 'edad': 1, 'an': 1, 'li': 1, 'hablada': 1, 'uelen': 1, 'hacer': 1, 'pau': 1, 'palabra': 4, 'lugar': 1, 'deben': 1, 'eparar': 1, 'menudo': 1, 'depende': 1, 'cu': 1, 'l': 2, 'po': 1, 'ibilidad': 1, 'mantenga': 1, 'entido': 1, 'gico': 1, 'gramatical': 1, 'contextual': 1, 'chino': 1, 'mandar': 1, 'tampoco': 1, 'eparacione': 1, 'Acento': 1, 'extranjero': 1, 'regionali': 1, 'dificultade': 1, 'producci': 1, 'habla': 1, 'errore': 2, 'mecanografiado': 1, 'expre': 1, 'ione': 1, 'gramaticale': 1, 'lectura': 1, 'texto': 1, 'mediante': 1, 'OCR': 1, 'principale': 1, 'tarea': 1}\n"]}]},{"cell_type":"code","source":["maximum_frequncy = max(word_frequencies.values()) # Obtenemos la frecuencia maxima\n","\n","for word in word_frequencies.keys(): # Iteramos todas las palabras del diccionario\n","    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy) # Actualizamos su frecuencia normalizandola al dividirla por el máximo. Ahora todas estaran el el rango (0-1]"],"metadata":{"id":"qwyY9TeWEEli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(word_frequencies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPfO0VpzEGSN","executionInfo":{"status":"ok","timestamp":1647793158776,"user_tz":-60,"elapsed":26,"user":{"displayName":"Daniel Ruiz Santamaría","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00829749213257681441"}},"outputId":"45f8647f-f9c2-40cc-849e-b09f60112e73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'El': 0.17391304347826086, 'proce': 0.21739130434782608, 'amiento': 0.17391304347826086, 'lenguaje': 0.30434782608695654, 'natural': 0.17391304347826086, 'abreviado': 0.043478260869565216, 'PLN': 0.2608695652173913, 'ingl': 0.08695652173913043, 'language': 0.043478260869565216, 'ing': 0.043478260869565216, 'NLP': 0.13043478260869565, 'campo': 0.043478260869565216, 'ciencia': 0.043478260869565216, 'computaci': 0.043478260869565216, 'n': 1.0, 'inteligencia': 0.08695652173913043, 'artificial': 0.043478260869565216, 'ling': 0.13043478260869565, 'tica': 0.43478260869565216, 'tudia': 0.043478260869565216, 'interaccione': 0.043478260869565216, 'computadora': 0.043478260869565216, 'humano': 0.08695652173913043, 'Se': 0.13043478260869565, 'ocupa': 0.043478260869565216, 'formulaci': 0.043478260869565216, 'inve': 0.13043478260869565, 'tigaci': 0.08695652173913043, 'mecani': 0.08695652173913043, 'mo': 0.13043478260869565, 'eficace': 0.08695652173913043, 'computacionalmente': 0.08695652173913043, 'comunicaci': 0.13043478260869565, 'per': 0.043478260869565216, 'ona': 0.043478260869565216, 'm': 0.17391304347826086, 'quina': 0.08695652173913043, 'medio': 0.17391304347826086, 'decir': 0.043478260869565216, 'lengua': 0.30434782608695654, 'mundo': 0.043478260869565216, 'No': 0.043478260869565216, 'trata': 0.043478260869565216, 'naturale': 0.13043478260869565, 'forma': 0.043478260869565216, 'ab': 0.043478260869565216, 'tracta': 0.043478260869565216, 'ino': 0.08695652173913043, 'di': 0.13043478260869565, 'ar': 0.043478260869565216, 'comunicar': 0.043478260869565216, 'ean': 0.043478260869565216, 'puedan': 0.043478260869565216, 'realizar': 0.043478260869565216, 'programa': 0.043478260869565216, 'ejecuten': 0.043478260869565216, 'imulen': 0.043478260869565216, 'Lo': 0.08695652173913043, 'modelo': 0.043478260869565216, 'aplicado': 0.043478260869565216, 'enfocan': 0.043478260869565216, 'olo': 0.08695652173913043, 'compren': 0.043478260869565216, 'i': 0.30434782608695654, 'pecto': 0.043478260869565216, 'generale': 0.043478260869565216, 'cognitivo': 0.043478260869565216, 'organizaci': 0.043478260869565216, 'memoria': 0.043478260869565216, 'irve': 0.043478260869565216, 'tudiar': 0.043478260869565216, 'to': 0.13043478260869565, 'fen': 0.043478260869565216, 'meno': 0.043478260869565216, 'Ha': 0.043478260869565216, 'ta': 0.08695652173913043, 'd': 0.043478260869565216, 'cada': 0.043478260869565216, 'mayor': 0.043478260869565216, 'tema': 0.13043478260869565, 'ba': 0.08695652173913043, 'aban': 0.043478260869565216, 'complejo': 0.043478260869565216, 'conjunto': 0.043478260869565216, 'regla': 0.08695652173913043, 'ada': 0.043478260869565216, 'mano': 0.08695652173913043, 'A': 0.043478260869565216, 'partir': 0.043478260869565216, 'finale': 0.08695652173913043, 'in': 0.13043478260869565, 'embargo': 0.043478260869565216, 'revoluci': 0.043478260869565216, 'introducci': 0.043478260869565216, 'algoritmo': 0.08695652173913043, 'aprendizaje': 0.13043478260869565, 'autom': 0.34782608695652173, 'tico': 0.13043478260869565, 'La': 0.13043478260869565, 'hi': 0.08695652173913043, 'toria': 0.08695652173913043, 'empieza': 0.043478260869565216, 'aunque': 0.043478260869565216, 'encontrado': 0.043478260869565216, 'trabajo': 0.08695652173913043, 'anteriore': 0.043478260869565216, 'En': 0.17391304347826086, 'Alan': 0.043478260869565216, 'Turing': 0.043478260869565216, 'public': 0.043478260869565216, 'Computing': 0.043478260869565216, 'machinery': 0.043478260869565216, 'and': 0.08695652173913043, 'intelligence': 0.043478260869565216, 'propon': 0.043478260869565216, 'hoy': 0.043478260869565216, 'llama': 0.043478260869565216, 't': 0.043478260869565216, 'turing': 0.043478260869565216, 'criterio': 0.043478260869565216, 'experimento': 0.043478260869565216, 'Georgetown': 0.043478260869565216, 'involucr': 0.043478260869565216, 'traducci': 0.2608695652173913, 'enta': 0.043478260869565216, 'oracione': 0.043478260869565216, 'ru': 0.043478260869565216, 'autore': 0.043478260869565216, 'tre': 0.043478260869565216, 'cinco': 0.043478260869565216, 'er': 0.043478260869565216, 'problema': 0.08695652173913043, 're': 0.17391304347826086, 'uelto': 0.043478260869565216, 'avance': 0.043478260869565216, 'real': 0.043478260869565216, 'lento': 0.043478260869565216, 'reporte': 0.043478260869565216, 'ALPAC': 0.043478260869565216, 'demo': 0.043478260869565216, 'tr': 0.043478260869565216, 'hab': 0.043478260869565216, 'bajo': 0.043478260869565216, 'empe': 0.043478260869565216, 'M': 0.043478260869565216, 'tarde': 0.043478260869565216, 'llevaron': 0.043478260869565216, 'cabo': 0.043478260869565216, 'tigacione': 0.043478260869565216, 'menor': 0.043478260869565216, 'cala': 0.043478260869565216, 'arrollaron': 0.043478260869565216, 'primero': 0.08695652173913043, 'tad': 0.043478260869565216, 'E': 0.043478260869565216, 'debi': 0.043478260869565216, 'aumento': 0.043478260869565216, 'tante': 0.043478260869565216, 'poder': 0.043478260869565216, 'c': 0.043478260869565216, 'mputo': 0.043478260869565216, 'ultante': 0.043478260869565216, 'ley': 0.043478260869565216, 'Moore': 0.043478260869565216, 'minuci': 0.043478260869565216, 'gradual': 0.043478260869565216, 'predominio': 0.043478260869565216, 'teor': 0.043478260869565216, 'Noam': 0.043478260869565216, 'Chom': 0.043478260869565216, 'ky': 0.043478260869565216, 'ejemplo': 0.043478260869565216, 'gram': 0.043478260869565216, 'tran': 0.043478260869565216, 'formacional': 0.043478260869565216, 'cuyo': 0.043478260869565216, 'fundamento': 0.043478260869565216, 'rico': 0.043478260869565216, 'alentaron': 0.043478260869565216, 'tipo': 0.08695652173913043, 'corpu': 0.043478260869565216, 'enfoque': 0.043478260869565216, 'u': 0.043478260869565216, 'aron': 0.043478260869565216, 'entonce': 0.08695652173913043, 'rbole': 0.08695652173913043, 'deci': 0.043478260869565216, 'producido': 0.043478260869565216, 'entencia': 0.043478260869565216, 'imilare': 0.043478260869565216, 'crita': 0.08695652173913043, 'puede': 0.043478260869565216, 'ultar': 0.043478260869565216, 'umen': 0.043478260869565216, 'publicacione': 0.043478260869565216, 'acerca': 0.043478260869565216, 'pu': 0.043478260869565216, 'proyecto': 0.043478260869565216, 'publicaci': 0.043478260869565216, 'doble': 0.043478260869565216, 'Frontier': 0.043478260869565216, 'Re': 0.043478260869565216, 'earch': 0.043478260869565216, 'Metric': 0.043478260869565216, 'Analytic': 0.043478260869565216, 'on': 0.08695652173913043, 'inherentemente': 0.043478260869565216, 'ambigua': 0.043478260869565216, 'diferente': 0.043478260869565216, 'nivele': 0.043478260869565216, 'Para': 0.043478260869565216, 'olver': 0.043478260869565216, 'ambig': 0.08695652173913043, 'edade': 0.043478260869565216, 'central': 0.043478260869565216, 'entrada': 0.043478260869565216, 'repre': 0.043478260869565216, 'entaci': 0.043478260869565216, 'interna': 0.043478260869565216, 'edad': 0.043478260869565216, 'an': 0.043478260869565216, 'li': 0.043478260869565216, 'hablada': 0.043478260869565216, 'uelen': 0.043478260869565216, 'hacer': 0.043478260869565216, 'pau': 0.043478260869565216, 'palabra': 0.17391304347826086, 'lugar': 0.043478260869565216, 'deben': 0.043478260869565216, 'eparar': 0.043478260869565216, 'menudo': 0.043478260869565216, 'depende': 0.043478260869565216, 'cu': 0.043478260869565216, 'l': 0.08695652173913043, 'po': 0.043478260869565216, 'ibilidad': 0.043478260869565216, 'mantenga': 0.043478260869565216, 'entido': 0.043478260869565216, 'gico': 0.043478260869565216, 'gramatical': 0.043478260869565216, 'contextual': 0.043478260869565216, 'chino': 0.043478260869565216, 'mandar': 0.043478260869565216, 'tampoco': 0.043478260869565216, 'eparacione': 0.043478260869565216, 'Acento': 0.043478260869565216, 'extranjero': 0.043478260869565216, 'regionali': 0.043478260869565216, 'dificultade': 0.043478260869565216, 'producci': 0.043478260869565216, 'habla': 0.043478260869565216, 'errore': 0.08695652173913043, 'mecanografiado': 0.043478260869565216, 'expre': 0.043478260869565216, 'ione': 0.043478260869565216, 'gramaticale': 0.043478260869565216, 'lectura': 0.043478260869565216, 'texto': 0.043478260869565216, 'mediante': 0.043478260869565216, 'OCR': 0.043478260869565216, 'principale': 0.043478260869565216, 'tarea': 0.043478260869565216}\n"]}]},{"cell_type":"markdown","source":["## Calculo de puntuaciones en las oraciones"],"metadata":{"id":"yS_Lt7neFEbo"}},{"cell_type":"code","source":["sentence_scores = {} # Diccionario de puntuación de frases del tipo frase --> Puntuacion\n","for sent in sentence_list: # Recorremos todas las frases\n","    for word in nltk.word_tokenize(sent.lower()): # Dividimos en palabras (tokenizamos), y las iteramos para cada frase\n","        if word in word_frequencies.keys(): # Si la palabra esta en el diccionario de frecuencias\n","            if len(sent.split(' ')) < 30: # Si hay menos de 30 palabras\n","                if sent not in sentence_scores.keys(): # Si no existia la frase en el diccionario de puntuación de frases\n","                    sentence_scores[sent] = word_frequencies[word] # La añado y pongo la puntuación de la  palabra\n","                else: # Si existia\n","                    sentence_scores[sent] += word_frequencies[word] # Sumo la puntuación a la acumulada para esa frase"],"metadata":{"id":"bL3wZXFxFIXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sentence_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KeVrT33DFLPT","executionInfo":{"status":"ok","timestamp":1647793158776,"user_tz":-60,"elapsed":24,"user":{"displayName":"Daniel Ruiz Santamaría","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00829749213257681441"}},"outputId":"aefd923a-8dae-4110-f24a-169e02fe1d99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Los modelos aplicados se enfocan no solo a la comprensión del lenguaje de por sí, sino a aspectos generales cognitivos humanos y a la organización de la memoria.': 0.3913043478260869, 'El lenguaje natural sirve solo de medio para estudiar estos fenómenos.': 0.6521739130434783, 'Hasta la década de 1980, la mayoría de los sistemas de PLN se basaban en un complejo conjunto de reglas diseñadas a mano.': 0.17391304347826086, 'A partir de finales de 1980, sin embargo, hubo una revolución en PLN con la introducción de algoritmos de aprendizaje automático para el procesamiento del lenguaje.': 0.5217391304347826, '\\u200b \\u200b La historia del PLN empieza desde 1950, aunque se han encontrado trabajos anteriores.': 0.13043478260869565, 'En 1950, Alan Turing publicó Computing machinery and intelligence, donde proponía lo que hoy se llama el test de turing como criterio de inteligencia.': 0.47826086956521735, 'En 1954, el experimento de Georgetown involucró traducción automática de más de sesenta oraciones del ruso al inglés.': 0.043478260869565216, 'Los autores sostuvieron que en tres o cinco años la traducción automática sería un problema resuelto.': 0.13043478260869565, 'El avance real en traducción automática fue más lento, y en 1966 el reporte ALPAC demostró que la investigación había tenido un bajo desempeño.': 0.21739130434782608, 'Más tarde, hasta finales de 1980, se llevaron a cabo investigaciones a menor escala en traducción automática, y se desarrollaron los primeros sistemas de traducción automática estadística.': 0.17391304347826086, 'Se usaron entonces los primeros algoritmos de aprendizaje automático, como los árboles de decisión, sistemas producidos de sentencias si-entonces similares a las reglas escritas a mano.': 0.21739130434782608, 'En la lengua hablada no se suelen hacer pausas entre palabra y palabra.': 0.7391304347826086, 'El lugar en el que se deben separar las palabras a menudo depende de cuál es la posibilidad de que mantenga un sentido lógico tanto gramatical como contextual.': 0.30434782608695654, 'En la lengua escrita, lenguas como el chino mandarín tampoco tienen separaciones entre las palabras.': 0.3913043478260869}\n"]}]},{"cell_type":"markdown","source":["## Obtención del resumen"],"metadata":{"id":"8bZPr7h_GmhP"}},{"cell_type":"code","source":["import heapq\n","summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n","\n","summary = '\\n\\t'.join(summary_sentences)\n","\n","print('Texto Original:\\n\\t',article_text)\n","\n","print('\\n\\nResumido:\\n\\t',summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fUHnr_q1GpvW","executionInfo":{"status":"ok","timestamp":1647793263511,"user_tz":-60,"elapsed":242,"user":{"displayName":"Daniel Ruiz Santamaría","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00829749213257681441"}},"outputId":"9af7c3e7-a0e2-41bc-ab4b-87ed129954f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Texto Original:\n","\t El procesamiento de lenguaje natural, ​ ​ abreviado PLN ​ ​ —en inglés, natural language processing, NLP— es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo. No trata de la comunicación por medio de lenguas naturales de una forma abstracta, sino de diseñar mecanismos para comunicarse que sean eficaces computacionalmente —que se puedan realizar por medio de programas que ejecuten o simulen la comunicación—. Los modelos aplicados se enfocan no solo a la comprensión del lenguaje de por sí, sino a aspectos generales cognitivos humanos y a la organización de la memoria. El lenguaje natural sirve solo de medio para estudiar estos fenómenos. Hasta la década de 1980, la mayoría de los sistemas de PLN se basaban en un complejo conjunto de reglas diseñadas a mano. A partir de finales de 1980, sin embargo, hubo una revolución en PLN con la introducción de algoritmos de aprendizaje automático para el procesamiento del lenguaje. ​ ​ La historia del PLN empieza desde 1950, aunque se han encontrado trabajos anteriores. En 1950, Alan Turing publicó Computing machinery and intelligence, donde proponía lo que hoy se llama el test de turing como criterio de inteligencia. En 1954, el experimento de Georgetown involucró traducción automática de más de sesenta oraciones del ruso al inglés. Los autores sostuvieron que en tres o cinco años la traducción automática sería un problema resuelto. El avance real en traducción automática fue más lento, y en 1966 el reporte ALPAC demostró que la investigación había tenido un bajo desempeño. Más tarde, hasta finales de 1980, se llevaron a cabo investigaciones a menor escala en traducción automática, y se desarrollaron los primeros sistemas de traducción automática estadística. Esto se debió tanto al aumento constante del poder de cómputo resultante de la ley de Moore como a la disminución gradual del predominio de las teorías lingüísticas de Noam Chomsky (por ejemplo, la gramática transformacional), cuyos fundamentos teóricos desalentaron el tipo de lingüística de corpus, que se basa en el enfoque de aprendizaje de máquinas para el procesamiento del lenguaje. Se usaron entonces los primeros algoritmos de aprendizaje automático, como los árboles de decisión, sistemas producidos de sentencias si-entonces similares a las reglas escritas a mano. Se puede consultar un resumen de la historia de 50 años de publicaciones acerca del procesamiento automático después del proyecto NLP4NLP en una publicación doble en Frontiers in Research Metrics and Analytics. ​ ​ Las lenguas naturales son inherentemente ambiguas en diferentes niveles: Para resolver estos tipos de ambigüedades y otros, el problema central en el PLN es la traducción de entradas en lenguas naturales a una representación interna sin ambigüedad, como árboles de análisis. En la lengua hablada no se suelen hacer pausas entre palabra y palabra. El lugar en el que se deben separar las palabras a menudo depende de cuál es la posibilidad de que mantenga un sentido lógico tanto gramatical como contextual. En la lengua escrita, lenguas como el chino mandarín tampoco tienen separaciones entre las palabras. Acentos extranjeros, regionalismos o dificultades en la producción del habla, errores de mecanografiado o expresiones no gramaticales, errores en la lectura de textos mediante OCR Las principales tareas de trabajo en el PLN son: \n","\n","\n","Resumido:\n","\t En la lengua hablada no se suelen hacer pausas entre palabra y palabra.\n","\tEl lenguaje natural sirve solo de medio para estudiar estos fenómenos.\n","\tA partir de finales de 1980, sin embargo, hubo una revolución en PLN con la introducción de algoritmos de aprendizaje automático para el procesamiento del lenguaje.\n","\tEn 1950, Alan Turing publicó Computing machinery and intelligence, donde proponía lo que hoy se llama el test de turing como criterio de inteligencia.\n","\tLos modelos aplicados se enfocan no solo a la comprensión del lenguaje de por sí, sino a aspectos generales cognitivos humanos y a la organización de la memoria.\n","\tEn la lengua escrita, lenguas como el chino mandarín tampoco tienen separaciones entre las palabras.\n","\tEl lugar en el que se deben separar las palabras a menudo depende de cuál es la posibilidad de que mantenga un sentido lógico tanto gramatical como contextual.\n"]}]}]}