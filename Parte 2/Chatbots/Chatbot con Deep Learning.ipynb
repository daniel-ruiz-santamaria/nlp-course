{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chatbot con Deep Learning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chat Bot con Deep Learning y NLP"],"metadata":{"id":"V9wMirSMCMTf"}},{"cell_type":"markdown","source":["## Instalaciones"],"metadata":{"id":"wpVcAB4pCRNN"}},{"cell_type":"code","source":["!pip install nltk\n","nltk.download('punkt')\n","!python -m spacy download es"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wd1P-pBHCTM4","executionInfo":{"status":"ok","timestamp":1649260919953,"user_tz":-120,"elapsed":15496,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"68cae8f9-3779-4f4e-e4dd-d3865cfa3afd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Collecting es_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n","\u001b[K     |████████████████████████████████| 16.2 MB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.63.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.21.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('es_core_news_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/es_core_news_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/es\n","You can now load the model via spacy.load('es')\n"]}]},{"cell_type":"markdown","source":["Importaciones"],"metadata":{"id":"OHEkWqq2CaIA"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem import SnowballStemmer\n","import spacy\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset,DataLoader\n","import random\n","import json"],"metadata":{"id":"PVf0uigYCcMD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Definicion de métodos para usar en el pipeline de procesamiento de texto"],"metadata":{"id":"jsljT_VmCiRd"}},{"cell_type":"markdown","source":["Incializacion de instancias"],"metadata":{"id":"P70K946cECHl"}},{"cell_type":"code","source":["stemmer = SnowballStemmer('spanish')\n","nlp_spacy = spacy.load('es')"],"metadata":{"id":"DQxivVfWEBhk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Limpieza de texto"],"metadata":{"id":"HuDTOQtvoUmv"}},{"cell_type":"code","source":["def limpiar(texto):\n","  import re\n","  texto = re.sub(r'[^ \\w+]', '', texto).strip()\n","  texto = re.sub(\"\\s\\s+\" , \" \", texto).strip()\n","  return texto.lower()"],"metadata":{"id":"PfVK7XjcoWwM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenización"],"metadata":{"id":"1oTtxcZdC0Oy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbDnIUWOyULD"},"outputs":[],"source":["def tokenizar(frase,nlp_spacy=nlp_spacy):\n","  doc = nlp_spacy(frase)\n","  tokens = [token.text for token in doc]\n","  return tokens"]},{"cell_type":"markdown","source":["Stemming"],"metadata":{"id":"uPCPE2nNC2Ks"}},{"cell_type":"code","source":["def stem(palabra,stemmer=stemmer):\n","  return stemmer.stem(palabra)"],"metadata":{"id":"duFyHfM3C4L9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bolsa de palabras"],"metadata":{"id":"f6Ib1tqyDAqo"}},{"cell_type":"code","source":["def bow(frase_tokenizada,lista_palabras):\n","  \"\"\"\n","  sentence = [\"hello\",\"how\",\"are\",\"you\"]\n","  words = [\"hi\",\"hell\",\"I\",\"you\",\"bye\",\"thank\",\"cool\"]\n","  bwg = [   0,     1,    0,   1,    0,     0,     0]\n","  \"\"\"\n","  stemmed = [stem(w) for w in frase_tokenizada] # Realizamos el stem sobre la frase\n","  bag = np.zeros(len(lista_palabras), dtype=np.float32) # Vector con todo 0s de la longitud de el diccionario\n","  for ind, palabra in enumerate(lista_palabras): # Para toda la lista de palabras\n","    if palabra in stemmed: # Si la palabra del diccionario aparece en la frase\n","      bag[ind] = 1.0 # Pongo el inidice a 1\n","  return bag"],"metadata":{"id":"t94F49wlDCl2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Carga de datos de entrenamiento"],"metadata":{"id":"iWfwlNKxRS2v"}},{"cell_type":"markdown","source":["Acceso a Drive"],"metadata":{"id":"KkTeM2XRRYnJ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","PATH_DATA= '/content/drive/MyDrive/nlp_introduction_course/data'\n","MODELS_DATA= '/content/drive/MyDrive/Colab Notebooks/nlp_introduction_course/models'\n","FILE_NAME = 'intents.json'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXprVwgTRjIW","executionInfo":{"status":"ok","timestamp":1649325169540,"user_tz":-120,"elapsed":2154,"user":{"displayName":"crishina mendes teixeira","userId":"02098891664090914434"}},"outputId":"e1a263d5-bd9d-4c3d-edff-a52e41e21305"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Leer los intents"],"metadata":{"id":"KxgA2xxKR3L7"}},{"cell_type":"code","source":["import json\n","with open(f'{PATH_DATA}/{FILE_NAME}','r') as f:\n","  intents = json.load(f)\n","print(intents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCcbJpOpR6Wg","executionInfo":{"status":"ok","timestamp":1649325173030,"user_tz":-120,"elapsed":860,"user":{"displayName":"crishina mendes teixeira","userId":"02098891664090914434"}},"outputId":"a63d2701-0e0c-487b-89cb-5894dc4e7cfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'intents': [{'tag': 'greeting', 'patterns': ['Hola', 'Hey', '¿Como estas?', '¿Hay alguien ahí?', 'Buenas', 'Buenos días'], 'responses': ['Hola', 'Hola, gracias por saludar', 'Hola, ¿En que puedo ayudarte?', 'Hola, ¿Que puedo hacer por ti?']}, {'tag': 'goodbye', 'patterns': ['Adios', 'Hasta luego', 'Bye Bye', 'Chao'], 'responses': ['Te veo despues, gracias', 'Que tengas un buen día', '¡Adios! Espero verte pronto.']}, {'tag': 'thanks', 'patterns': ['Gracias', 'Muchas gracias', 'Me has ayudado', 'gracias por ayudarme'], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}, {'tag': 'items', 'patterns': ['¿Que articulos tienes?', '¿Que vendes?', '¿Que puedes venderme?', '¿Que puedo comprar?'], 'responses': ['Vendo cofe y té', 'Puedo vender cafe y té']}, {'tag': 'payments', 'patterns': ['¿Que metodos de pago aceptas?', '¿Aceptas tarjeta de credito?', '¿Puedo pagar con tarjeta de credito?', '¿Aceptas Visa?', '¿Puedo pagar con Visa?', '¿Aceptas Mastercar?', '¿Puedo pagar con Mastercar?', '¿Aceptas American Express?', '¿Puedo pagar con American Express?', '¿Aceptas Paypal?', '¿Puedo pagar con Paypal?', '¿Aceptas efectivo?', '¿Aceptas dinero?', '¿Aceptas cash?'], 'responses': ['Solo acepto VISA, Mastercard y Paypal como metodos de pago', 'Acepto la mayoria de tarjetas de credito y Paypal']}, {'tag': 'delivery', 'patterns': ['¿Cuanto tarda la entrega?', '¿Cuanto tarda el envio entrega?', '¿Cuando tendre mi pedido?'], 'responses': ['La entrega tarda de 2 a 4 días', 'En 2 o 4 días, lo tienes en tu casa']}, {'tag': 'funny', 'patterns': ['Chiste', 'Cuentame un chiste', '¿Sabes algun chiste?'], 'responses': ['¿Cuál es el último animal que subió al arca de Noé? El del-fin', 'Camarero, ese filete tiene muchos nervios. Pues normal, es la primera vez que se lo comen', '¿Qué le dice una iguana a su hermana gemela? Somos iguanitas', 'Mamá, mamá, ¿sabías que Juana de Arco era drogadicta? La mamá le mire y le dice: ¿Pero qué dices? Eso no es cierto. Que sí, mamá, en el libro pone que murió por heroína', '¿Cómo se llama el campeón de apnea japonés? Tokofondo, ¿y el subcampeón? Kasitoko.', 'Una mujer le dice a su marido: Cariño, ¿te gusta mi disfraz? Sí, mi amor, contesta el hombre, es un disfraz de vaca muy bonito. ¡Pero si voy disfrazada de dálmata!']}]}\n"]}]},{"cell_type":"markdown","source":["Vamos a crear una fucion para aplicar Limpieza, tokenizacion y Stemming y obtener la lista de palabras, la lista de tags, y los conjuntos de entrenamiento en forma de tuplas (tokens, tag)"],"metadata":{"id":"IYuoEpYtT06_"}},{"cell_type":"code","source":["def preparar_datos(intents):\n","  lista_palabras = [] # Lista de palabras\n","  tags = [] # Intenciones\n","  xy = [] # Conjunto de entrenamiento en forma de tuplas (tokens, tag)\n","  for intent in intents['intents']:\n","    tag = intent['tag']\n","    tags.append(tag) # Añadimos el contenido de tag\n","    for patron in intent['patterns']: # Por cada patron\n","      patron = limpiar(patron) # Limpio pasando a minusculas, quitando signos de puntuación y espacios\n","      tokens = tokenizar(patron) # Tokenizo y obtengo la lista de palabras\n","      tokens_stemmed = [stem(t) for t in tokens]\n","      lista_palabras.extend(tokens_stemmed) # Añado la lista de palabras\n","      xy.append((tokens, tag)) # Para el conjunto de entrenamiento añado la tupla forma de tuplas (tokens, tag)\n","  lista_palabras = sorted(set(lista_palabras))# Dejamos unicamente las palabras únicas, y las ponemos de forma ordenada\n","  tags = sorted(set(tags)) # Ordenamos los tags\n","  x_train = []\n","  y_train = []\n","  for (tokens,tag) in xy:\n","    bag = bow(tokens,lista_palabras) # Calculamos la bolsa de palabras de la frase\n","    x_train.append(bag) # Lo añadimos a la lista de valores de entrenamiento\n","\n","    label = tags.index(tag) # Obtenemos el indice de la label\n","    y_train.append(label) # Lo añadimos a los valores a predecir\n","  \n","  return lista_palabras,tags,np.array(x_train),np.array(y_train) # Retornamos lista_palabras, tags, x_train y y_train"],"metadata":{"id":"x4nuL-LxT4Xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_palabras,tags,x_train,y_train = preparar_datos(intents)"],"metadata":{"id":"iTVh2dy5teh0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creamos objeto para gestionar el dataset"],"metadata":{"id":"efn1-1836NPp"}},{"cell_type":"code","source":["class ChatDataset(Dataset):\n","\n","    def __init__(self):\n","        self.n_samples = len(x_train)\n","        self.x_data = x_train\n","        self.y_data = y_train\n","\n","    # support indexing such that dataset[i] can be used to get i-th sample\n","    def __getitem__(self, index):\n","        return self.x_data[index], self.y_data[index]\n","\n","    # we can call len(dataset) to return the size\n","    def __len__(self):\n","        return self.n_samples"],"metadata":{"id":"xSgCRnq56bWb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hiperparametros"],"metadata":{"id":"_GQdOduw9Ozg"}},{"cell_type":"code","source":["batch_size = 8"],"metadata":{"id":"JGUZvxeHIljR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = ChatDataset()\n","train_loader = DataLoader(dataset=dataset,\n","                          batch_size=batch_size,\n","                          shuffle=True,\n","                          num_workers=0)"],"metadata":{"id":"bKn3STNy8wfb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Modelo PyTorch y entrenamiento"],"metadata":{"id":"jQtDd4Q7-HpO"}},{"cell_type":"markdown","source":["Creamos una clase para el modelo"],"metadata":{"id":"Cjamm30n-cCC"}},{"cell_type":"code","source":["class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.l1 = nn.Linear(input_size, hidden_size) \n","        self.l2 = nn.Linear(hidden_size, hidden_size) \n","        self.l3 = nn.Linear(hidden_size, num_classes)\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","        out = self.l1(x)\n","        out = self.relu(out)\n","        out = self.l2(out)\n","        out = self.relu(out)\n","        out = self.l3(out)\n","        # no activation and no softmax at the end\n","        return out"],"metadata":{"id":"lsJAATyQ-fwo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La instanciamos"],"metadata":{"id":"BvzrzQNdA65A"}},{"cell_type":"code","source":["# Hyper-parameters \n","num_epochs = 2000\n","batch_size = 8\n","learning_rate = 0.001\n","input_size = len(lista_palabras)\n","hidden_size = 8\n","output_size = len(tags)\n","print(input_size, output_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r9sR32CPB_qu","executionInfo":{"status":"ok","timestamp":1649270931658,"user_tz":-120,"elapsed":374,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"381e42e3-3629-477f-b4a2-fe01ce10bce1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["56 7\n"]}]},{"cell_type":"markdown","source":["Bucle de entrenamiento"],"metadata":{"id":"c7hfoMc0Eu6o"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Usamos si podemos CUDA si no cpu\n","model = NeuralNet(input_size, hidden_size, output_size).to(device) # Inicializamos el modelo\n","criterion = nn.CrossEntropyLoss() # Funcion de perdida CrossEntropyLoss\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Optimizador Adam\n","for epoch in range(num_epochs): # Para todas las epocas\n","    for (words, labels) in train_loader: # Datos de entrenamiento\n","        words = words.to(device)\n","        labels = labels.to(dtype=torch.long).to(device)\n","        \n","        # Forward pass\n","        outputs = model(words)\n","        # if y would be one-hot, we must apply\n","        # labels = torch.max(labels, 1)[1]\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    if (epoch+1) % 100 == 0:\n","        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","    if loss.item() < 0.0001:\n","      break\n","\n","\n","print(f'final loss: {loss.item():.6f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8bHQzaFEwxz","executionInfo":{"status":"ok","timestamp":1649270937444,"user_tz":-120,"elapsed":4265,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"7aa5e21d-19db-4d42-db20-5f1deacea688"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [100/2000], Loss: 0.5372\n","Epoch [200/2000], Loss: 0.2025\n","Epoch [300/2000], Loss: 0.0080\n","Epoch [400/2000], Loss: 0.0131\n","Epoch [500/2000], Loss: 0.0012\n","Epoch [600/2000], Loss: 0.0011\n","Epoch [700/2000], Loss: 0.0021\n","Epoch [800/2000], Loss: 0.0001\n","final loss: 0.000052\n"]}]},{"cell_type":"markdown","source":["Guardamos el modelo"],"metadata":{"id":"1XBaFusILaJm"}},{"cell_type":"code","source":["data = {\n","  \"model_state\": model.state_dict(),\n","  \"input_size\": input_size,\n","  \"hidden_size\": hidden_size,\n","  \"output_size\": output_size,\n","  \"all_words\": lista_palabras,\n","  \"tags\": tags\n","}"],"metadata":{"id":"oUvtBbX0Lc3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_FILE = \"chatbot_data.pth\"\n","torch.save(data, f'{MODELS_DATA}/{MODEL_FILE}')\n","\n","print(f'training complete. file saved to {MODELS_DATA}/{MODEL_FILE}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oj1zG-iLrBU","executionInfo":{"status":"ok","timestamp":1649270945155,"user_tz":-120,"elapsed":365,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"ecb9371f-be5f-462a-81b5-ebe262e85fff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["training complete. file saved to /content/drive/MyDrive/Colab Notebooks/nlp_introduction_course/models/chatbot_data.pth\n"]}]},{"cell_type":"markdown","source":["# Chatbot"],"metadata":{"id":"71YFEdVgM3wT"}},{"cell_type":"markdown","source":["Cargamos el modelo"],"metadata":{"id":"i8lgzHBaM-Bw"}},{"cell_type":"code","source":["data = torch.load(f'{MODELS_DATA}/{MODEL_FILE}')"],"metadata":{"id":"GFUss265MFvI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Actualizamos los parametros desde data"],"metadata":{"id":"h5jcyaTlNff9"}},{"cell_type":"code","source":["input_size = data[\"input_size\"]\n","hidden_size = data[\"hidden_size\"]\n","output_size = data[\"output_size\"]\n","all_words = data['all_words']\n","tags = data['tags']\n","model_state = data[\"model_state\"]"],"metadata":{"id":"h17_PoZ1NjC8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cargamos los intents"],"metadata":{"id":"3JiJPaBRNwYX"}},{"cell_type":"code","source":["with open(f'{PATH_DATA}/{FILE_NAME}', 'r') as json_data:\n","    intents = json.load(json_data)"],"metadata":{"id":"pwWXoqT4Nyc9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instanciamos el modelo desde el diccionario con los pesos de la red neuronal"],"metadata":{"id":"_qHzwFEWN_dV"}},{"cell_type":"code","source":["model = NeuralNet(input_size, hidden_size, output_size).to(device)\n","model.load_state_dict(model_state)\n","model.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uXAWgakNN_kr","executionInfo":{"status":"ok","timestamp":1649271153713,"user_tz":-120,"elapsed":335,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"7e70edbc-1573-4eb8-fb37-b8049b64eb68"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NeuralNet(\n","  (l1): Linear(in_features=56, out_features=8, bias=True)\n","  (l2): Linear(in_features=8, out_features=8, bias=True)\n","  (l3): Linear(in_features=8, out_features=7, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":299}]},{"cell_type":"markdown","source":["Bucle infinito para el Chat"],"metadata":{"id":"TUP5aaqaOMiw"}},{"cell_type":"code","source":["bot_name = \"Bot\"\n","print(f\"{bot_name}: Vamos a chatear! (escribe 'salir' para salir)\")\n","while True:\n","    sentence = input(\"You: \")\n","    if sentence == \"salir\":\n","        print('...Saliendo XD')\n","        break\n","\n","    sentence = tokenizar(sentence)\n","    X = bow(sentence, all_words)\n","    X = X.reshape(1, X.shape[0])\n","    X = torch.from_numpy(X).to(device)\n","\n","    output = model(X)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    tag = tags[predicted.item()]\n","\n","    probs = torch.softmax(output, dim=1)\n","    prob = probs[0][predicted.item()]\n","    if prob.item() > 0.75:\n","        for intent in intents['intents']:\n","            if tag == intent[\"tag\"]:\n","                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n","    else:\n","        print(f\"{bot_name}: Perdona no te he entendido...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPT77iPMOSwX","executionInfo":{"status":"ok","timestamp":1649271548753,"user_tz":-120,"elapsed":3797,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"efffcce9-ece6-41a5-ceaa-ed62d4d40fcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bot: Vamos a chatear! (escribe 'salir' para salir)\n","You: salir\n","...Saliendo XD\n"]}]}]}