{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Normalización.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNpJtKJaCsjXUnKXhNr6H1u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Normalización"],"metadata":{"id":"j0N5FyuP1dme"}},{"cell_type":"markdown","source":["Las tecnicas de normalización intentan reducir la complejidad derivada de:\n","- El uso de sinonimos\n","- La existencia de palabras derivadas de una misma raiz\n","- Las variaciones de las palabras en cuanto a genero y numero (para nombres) y conjugaciones (para formas verbales)"],"metadata":{"id":"KAAohXNm1iQ1"}},{"cell_type":"code","source":["import nltk # Importacion de nltk\n","nltk.download('wordnet') # Descarga del paquete wordnet. WordNet es una BBDD, de palabras, de la que entre otras cosas, podemos obtener sinonimos y antonimos.\n","from nltk.corpus import wordnet # Importación de wordnet\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DBLPLBmRC3F","executionInfo":{"status":"ok","timestamp":1649607485717,"user_tz":-120,"elapsed":602,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"0eae7dcb-c651-4ad9-ad79-a3223d64754a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"markdown","source":["## Sinonimos"],"metadata":{"id":"Gi20sda02C8s"}},{"cell_type":"markdown","source":["Funcion para encontrar sinonimos"],"metadata":{"id":"Q5RUZ9hJie9u"}},{"cell_type":"code","source":["# Función para obtener sinonimos de una palabra\n","def get_synonyms(palabra):\n","  synonyms = [] # Lista de sinonimos\n","\n","  for syn in wordnet.synsets(palabra): # Obtenemos la lista de palabras realcionadas, llamando a la función synsets de wordnet, e iteramos por cada una de ellas\n","      for l in syn.lemmas(): # Obtener los lemas de las palabras relacionadas\n","        if not l.antonyms(): # Si no es antonimo\n","          synonyms.append(l.name()) # Insertarlas en la lista\n","  return synonyms\n"],"metadata":{"id":"YzF79a-viZe_","executionInfo":{"status":"ok","timestamp":1649608131440,"user_tz":-120,"elapsed":257,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["palabra = input(\"Ingresa una palabra en ingles para buscar sinonimos\\n\")\n","print(get_synonyms(palabra))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjO8uLabi38K","executionInfo":{"status":"ok","timestamp":1649608137939,"user_tz":-120,"elapsed":2669,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"a2774f66-6137-427f-d633-29bf780dda7f"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Ingresa una palabra en ingles para buscar sinonimos\n","love\n","['love', 'passion', 'beloved', 'dear', 'dearest', 'honey', 'love', 'love', 'sexual_love', 'erotic_love', 'love', 'sexual_love', 'lovemaking', 'making_love', 'love', 'love_life', 'love', 'enjoy', 'love', 'sleep_together', 'roll_in_the_hay', 'love', 'make_out', 'make_love', 'sleep_with', 'get_laid', 'have_sex', 'know', 'do_it', 'be_intimate', 'have_intercourse', 'have_it_away', 'have_it_off', 'screw', 'fuck', 'jazz', 'eff', 'hump', 'lie_with', 'bed', 'have_a_go_at_it', 'bang', 'get_it_on', 'bonk']\n"]}]},{"cell_type":"markdown","source":["Función para encontrar antonimos"],"metadata":{"id":"pFq8bikNpwyG"}},{"cell_type":"code","source":["# Función para obtener sinonimos de una palabra\n","def get_antonyms(palabra):\n","  from nltk.corpus import wordnet\n","  antonyms  = [] # Lista de antonimos\n","\n","  for syn in wordnet.synsets(palabra): # Obtenemos la lista de palabras realcionadas, llamando a la función synsets de wordnet, e iteramos por cada una de ellas\n","      for l in syn.lemmas(): # Obtener los lemas de las palabras relacionadas\n","         if l.antonyms(): # Si  es antonimo\n","              antonyms.append(l.antonyms()[0].name()) # Insertarlas en la lista\n","  return antonyms"],"metadata":{"id":"qyLnxqoEp189","executionInfo":{"status":"ok","timestamp":1649608218750,"user_tz":-120,"elapsed":257,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["palabra = input(\"Ingresa una palabra en ingles para buscar antonimos\\n\")\n","print(get_antonyms(palabra))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GCxhoYaqHd-","executionInfo":{"status":"ok","timestamp":1649608221609,"user_tz":-120,"elapsed":2166,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"1cd933a7-f086-4fd3-8223-dcb4dcc84e5c"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Ingresa una palabra en ingles para buscar antonimos\n","love\n","['hate', 'hate']\n"]}]},{"cell_type":"markdown","source":["Ahora la idea seria sustituir en una frase todos los sinonimos de una cierta palabra, siempre por la misma palabra, de forma que se reduzca la complejidad"],"metadata":{"id":"EmHQvnotq2vk"}},{"cell_type":"code","source":["palabra_principal = \"love\"\n","frase = \"Love is one of the most important things. Passion and honey are important\""],"metadata":{"id":"7QqpgYnHq0z_","executionInfo":{"status":"ok","timestamp":1649608233262,"user_tz":-120,"elapsed":234,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Función para sustituir lso sinonimos en una frase, de una palabra principal\n","def remplazar_por_sinonimos(palabra_principal,frase): # Recibe como parametro la palabra principal, y la frase\n","  synonyms = get_synonyms(palabra_principal) # Obtengo la lista de sinonimos de la palabra principal\n","  nueva_frase = [] # Nueva frase con la sustitución por sinonimos\n","  for word in frase.lower().split(\" \"): # Divido en palabras e itero por ellas\n","    if word  in synonyms: # Si la palabra, esta en la lista de sinonimos\n","      nueva_frase.append(palabra_principal) # Meto en su lugar la palabra principal\n","    else: #Si no esta\n","      nueva_frase.append(word) # Metemos la palabra\n","  return \" \".join(nueva_frase) # Retornamos la nueva frase, con los sinonimos sustituidos"],"metadata":{"id":"j3YCnST8sENc","executionInfo":{"status":"ok","timestamp":1649607516457,"user_tz":-120,"elapsed":240,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["nueva_frase = remplazar_por_sinonimos(palabra_principal,frase)\n","print(nueva_frase)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEtFM_RVsY65","executionInfo":{"status":"ok","timestamp":1649608423207,"user_tz":-120,"elapsed":249,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"ccb5e047-25d5-454b-d0ed-563b2412a975"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["love is one of the most important things. love and love are important\n"]}]},{"cell_type":"markdown","source":["## Derivación regresiva o Stemming"],"metadata":{"id":"djhx3_D3tvxX"}},{"cell_type":"markdown","source":["La derivación regresiva consiste en encontrar la parte fija de la palabra, excluyendo prefijos y sufijos"],"metadata":{"id":"MFNijVCVuTTW"}},{"cell_type":"markdown","source":["#### En ingles"],"metadata":{"id":"Gul4bdRquV5G"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer # Usamos la clase PorterStemmer, \n","\n","stemmer = PorterStemmer() # Cremos una instancia de la clase\n","\n","print(stemmer.stem('working')) # Perdimos la palabra raiz, eliminado sufijos y prefijos\n","print(stemmer.stem('worked'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkG4soU7uZbE","executionInfo":{"status":"ok","timestamp":1649608672466,"user_tz":-120,"elapsed":285,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"8d04306c-18a1-4cd7-df83-415f0f95cd00"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["work\n","work\n"]}]},{"cell_type":"markdown","source":["#### En español"],"metadata":{"id":"8bsjEEKEuePa"}},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer # Usamos la clase SnowballStemmer, \n","\n","spanish_stemmer = SnowballStemmer('spanish')  # Cremos una instancia de la clase, recibiendo como parametro el idioma\n","\n","print(spanish_stemmer.stem('trabajando')) # Perdimos la palabra raiz, eliminado sufijos y prefijos\n","print(spanish_stemmer.stem('trabajo'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DwYptb5oujbi","executionInfo":{"status":"ok","timestamp":1649608581430,"user_tz":-120,"elapsed":244,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"d7da194a-47c1-4f1a-df09-1a9c509a88fd"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["trabaj\n","trabaj\n"]}]},{"cell_type":"markdown","source":["Función para obtener la raiz de una palabra a partir de una frase en cualquier idioma"],"metadata":{"id":"yF8si7enuu1E"}},{"cell_type":"code","source":["# Creamos una función para obtener la raiz de las palabras de una frase\n","def obtener_raiz(frase,idioma='english' ): # recibimos como parámetros la frase y el idioma, que por defecto sera en ingles\n","  from nltk.stem import SnowballStemmer\n","  stemmer = SnowballStemmer(idioma) #  Cremos una instancia de la clase, recibiendo como parametro el idioma \n","  clean_token_sin_strems = [stemmer.stem(token) for token in frase.split(\" \")] # Dividimos en palabras separando por espacios en blanco, y aplicamos la funcion stem a cada palabra, para obtener la raiz\n","  return clean_token_sin_strems # Retornamos las palabras de la frase, despues de aplciar el stemmer"],"metadata":{"id":"wuR0D5CRu20i","executionInfo":{"status":"ok","timestamp":1649607533664,"user_tz":-120,"elapsed":434,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["obtener_raiz(\"Hoy estoy cansado por que he estado trabajando todo el día\",\"spanish\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KsPTpfFvS-C","executionInfo":{"status":"ok","timestamp":1649607535019,"user_tz":-120,"elapsed":5,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"d0d8168e-73b7-4d3c-d16a-9871f2ecf7b7"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hoy',\n"," 'estoy',\n"," 'cans',\n"," 'por',\n"," 'que',\n"," 'he',\n"," 'estad',\n"," 'trabaj',\n"," 'tod',\n"," 'el',\n"," 'dia']"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["obtener_raiz(\"La trabajadora esta cansada por haber trabajado todos los días\",\"spanish\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Yqc5rE8l5Nr","executionInfo":{"status":"ok","timestamp":1649608922826,"user_tz":-120,"elapsed":237,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"6f2ddfff-8d21-4d47-8340-ab946b4026cf"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['la', 'trabaj', 'esta', 'cans', 'por', 'hab', 'trabaj', 'tod', 'los', 'dias']"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["# Lematización"],"metadata":{"id":"0tDDtWJQvoey"}},{"cell_type":"markdown","source":["A partir de las formas flexionadas de las palabras, permite obtener la forma canónica de una palabra excluyendo el genero, numero, conjugación verbal….. Por ejemplo decir es la forma canonica de dire, dije, dijésemos …"],"metadata":{"id":"548VOaIQv_U6"}},{"cell_type":"markdown","source":["#### En Ingles"],"metadata":{"id":"P-8gtO2UwRfq"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer # En nltk, importamos la clase WordNetLemmatizer, que usa WordNet, para conocer la raiz de la palabra\n","lematizer = WordNetLemmatizer() # Instanciamos la clase\n","print(lematizer.lemmatize('was', pos='v')) # Sustantivos de manera predeterminada, especificar \"v\" si se quiere trabajar con verbos"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGLAftFewLzX","executionInfo":{"status":"ok","timestamp":1649608985945,"user_tz":-120,"elapsed":256,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"3089acb1-d0f1-4ac0-b1c9-60f9007c33e8"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["be\n"]}]},{"cell_type":"markdown","source":["#### En Español"],"metadata":{"id":"IGYfDdVowYxx"}},{"cell_type":"code","source":["!python -m spacy download es_core_news_sm\n","!python -m spacy link es_core_news_sm es\n","import spacy\n","nlp = spacy.load('es')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZzViqjjRcdj","executionInfo":{"status":"ok","timestamp":1649609006633,"user_tz":-120,"elapsed":8970,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"955021ca-68f9-4db7-be54-0f0416eb9f04"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting es_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n","\u001b[K     |████████████████████████████████| 16.2 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.21.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.63.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('es_core_news_sm')\n","\n","\u001b[38;5;1m✘ Link 'es' already exists\u001b[0m\n","To overwrite an existing link, use the --force flag\n","\n"]}]},{"cell_type":"code","source":["\n","frase_1 = \"Siempre me ha gustado leer grandes libros\"\n","frase_2 = \"Me gustan los libros aunque ya los haya leído\"\n","\n","def lematizar(frase):\n","  clean_tokens_sin_lem = [] # Lista donde pondremos las palabras lematizadas\n","  separator = ' ' # Separador usado\n","  for token in frase.split(separator): # Separamos la frase en palabras\n","      for word in nlp(token): # Pasamos cada palabra al modelo de Spacy\n","          print('\\t'+word.text+ ' ==> ' +word.lemma_+ ' ==> ' +word.pos_) # Obtenemos el texto original con \"text\", el lema con \"lemma_\" y la función en la frase con \"pos_\", y lo mostramos por pantalla\n","          clean_tokens_sin_lem.append(word.lemma_) # Insertamos el lema en la lista\n","  return clean_tokens_sin_lem # Retornamos la Lista de palabras lematizadas\n","print('Frase 1 Lematizada',lematizar(frase_1)) # Ejecutamos con la frase 1\n","print('Frase 2 Lematizada',lematizar(frase_2)) # Ejecutamos con la frase 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMbhNfl0gOzu","executionInfo":{"status":"ok","timestamp":1649609370707,"user_tz":-120,"elapsed":253,"user":{"displayName":"Daniel Ruiz Santamaría","userId":"00829749213257681441"}},"outputId":"a5516d19-8dad-4439-9cd3-242ce7c7a2e4"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["\tSiempre ==> Siempre ==> ADV\n","\tme ==> me ==> PRON\n","\tha ==> haber ==> AUX\n","\tgustado ==> gustar ==> ADJ\n","\tleer ==> leer ==> VERB\n","\tgrandes ==> grande ==> ADJ\n","\tlibros ==> libro ==> NOUN\n","Frase 1 Lematizada ['Siempre', 'me', 'haber', 'gustar', 'leer', 'grande', 'libro']\n","\tMe ==> Me ==> PRON\n","\tgustan ==> gustar ==> VERB\n","\tlos ==> lo ==> DET\n","\tlibros ==> libro ==> NOUN\n","\taunque ==> aunque ==> SCONJ\n","\tya ==> ya ==> ADV\n","\tlos ==> lo ==> DET\n","\thaya ==> haber ==> AUX\n","\tleído ==> leer ==> ADJ\n","Frase 2 Lematizada ['Me', 'gustar', 'lo', 'libro', 'aunque', 'ya', 'lo', 'haber', 'leer']\n"]}]}]}